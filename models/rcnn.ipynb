{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "BATCH_SIZE = 4 # increase / decrease according to GPU memeory\n",
    "RESIZE_TO = 512 # resize the image for training and transforms\n",
    "NUM_EPOCHS = 10 # number of epochs to train for\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# training images and XML files directory\n",
    "TRAIN_DIR = '../data/rcnn/training'\n",
    "# validation images and XML files directory\n",
    "VALID_DIR = '../data/rcnn/testing'\n",
    "\n",
    "# classes: 0 index is reserved for background\n",
    "CLASSES = [\n",
    "    'background', 'abw', 'pbw'\n",
    "]\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# whether to visualize images after crearing the data loaders\n",
    "VISUALIZE_TRANSFORMED_IMAGES = False\n",
    "\n",
    "# location to save model and plots\n",
    "OUT_DIR = '../results/rcnn'\n",
    "SAVE_PLOTS_EPOCH = 2 # save loss plots after these many epochs\n",
    "SAVE_MODEL_EPOCH = 2 # save model after these many epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PILLOW_VERSION' from 'PIL' (/usr/local/Caskroom/miniforge/base/envs/vision-next/lib/python3.9/site-packages/PIL/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39malbumentations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpytorch\u001b[39;00m \u001b[39mimport\u001b[39;00m ToTensorV2\n\u001b[1;32m      7\u001b[0m \u001b[39m# this class keeps track of the training and validation loss values...\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# ... and helps to get the average for each epoch as well\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAverager\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision-next/lib/python3.9/site-packages/albumentations/pytorch/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m absolute_import\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision-next/lib/python3.9/site-packages/albumentations/pytorch/transforms.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms_interface\u001b[39;00m \u001b[39mimport\u001b[39;00m BasicTransform\n\u001b[1;32m     11\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mToTensorV2\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision-next/lib/python3.9/site-packages/torchvision/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision-next/lib/python3.9/site-packages/torchvision/datasets/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msvhn\u001b[39;00m \u001b[39mimport\u001b[39;00m SVHN\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mphototour\u001b[39;00m \u001b[39mimport\u001b[39;00m PhotoTour\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfakedata\u001b[39;00m \u001b[39mimport\u001b[39;00m FakeData\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msemeion\u001b[39;00m \u001b[39mimport\u001b[39;00m SEMEION\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39momniglot\u001b[39;00m \u001b[39mimport\u001b[39;00m Omniglot\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision-next/lib/python3.9/site-packages/torchvision/datasets/fakedata.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdata\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[1;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mFakeData\u001b[39;00m(data\u001b[39m.\u001b[39mDataset):\n\u001b[1;32m      7\u001b[0m     \u001b[39m\"\"\"A fake dataset that returns randomly generated images and returns them as PIL images\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision-next/lib/python3.9/site-packages/torchvision/transforms/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision-next/lib/python3.9/site-packages/torchvision/transforms/transforms.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcollections\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info \u001b[39m<\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[1;32m     20\u001b[0m     Sequence \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mSequence\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision-next/lib/python3.9/site-packages/torchvision/transforms/functional.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image, ImageOps, ImageEnhance, PILLOW_VERSION\n\u001b[1;32m      6\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39maccimage\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PILLOW_VERSION' from 'PIL' (/usr/local/Caskroom/miniforge/base/envs/vision-next/lib/python3.9/site-packages/PIL/__init__.py)"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# this class keeps track of the training and validation loss values...\n",
    "# ... and helps to get the average for each epoch as well\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    To handle the data loading as different images may have different number \n",
    "    of objects and to handle varying size tensors as well.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training tranforms\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        A.RandomRotate90(0.5),\n",
    "        A.MotionBlur(p=0.2),\n",
    "        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        A.Blur(blur_limit=3, p=0.1),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc',\n",
    "        'label_fields': ['labels']\n",
    "    })\n",
    "\n",
    "# define the validation transforms\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc', \n",
    "        'label_fields': ['labels']\n",
    "    })\n",
    "\n",
    "# define the negative sample transforms\n",
    "def get_negative_sample_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tranformed_image(train_loader):\n",
    "    \"\"\"\n",
    "    This function shows the transformed images from the `train_loader`.\n",
    "    Helps to check whether the tranformed images along with the corresponding\n",
    "    labels are correct or not.\n",
    "    Only runs if `VISUALIZE_TRANSFORMED_IMAGES = True` in config.py.\n",
    "    \"\"\"\n",
    "    if len(train_loader) > 0:\n",
    "        for i in range(1):\n",
    "            images, targets = next(iter(train_loader))\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "            boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "            sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            for box in boxes:\n",
    "                cv2.rectangle(sample,\n",
    "                            (box[0], box[1]),\n",
    "                            (box[2], box[3]),\n",
    "                            (0, 0, 255), 2)\n",
    "            cv2.imshow('Transformed image', sample)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset class\n",
    "class WormDataset(Dataset):\n",
    "    def __init__(self, dir_path, width, height, classes, transforms, negative_sample_transforms):\n",
    "        self.transforms = transforms\n",
    "        self.negative_sample_transforms = negative_sample_transforms\n",
    "        self.dir_path = dir_path\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.classes = classes\n",
    "        \n",
    "        # get all the image paths in sorted order\n",
    "        self.image_paths = glob.glob(f\"{self.dir_path}/*[jpg|jpeg]\")\n",
    "        self.all_images = [image_path.split('/')[-1] for image_path in self.image_paths]\n",
    "        self.all_images = sorted(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # capture the image name and the full image path\n",
    "        image_name = self.all_images[idx]\n",
    "        image_path = os.path.join(self.dir_path, image_name)\n",
    "\n",
    "        # read the image\n",
    "        image = cv2.imread(image_path)\n",
    "        # convert BGR to RGB color format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image_resized = cv2.resize(image, (self.width, self.height))\n",
    "        image_resized /= 255.0\n",
    "        \n",
    "        # capture the corresponding XML file for getting the annotations\n",
    "        annotations_file = image_name.split('.')[0] + '.txt'\n",
    "        annotations_file_path = os.path.join(self.dir_path, annotations_file)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        # get the height and width of the image\n",
    "        image_width = image.shape[1]\n",
    "        image_height = image.shape[0]\n",
    "        \n",
    "        # box coordinates for xml files are extracted and corrected for image size given\n",
    "\n",
    "        bounding_boxes_df = pd.read_csv(annotations_file_path, sep='\\t', names=[\"target\", 'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "        bounding_boxes_df['xmin'] = bounding_boxes_df['xmin'] * self.width\n",
    "        bounding_boxes_df['xmax'] = bounding_boxes_df['xmax'] * self.width\n",
    "        bounding_boxes_df['ymin'] = bounding_boxes_df['ymin'] * self.height\n",
    "        bounding_boxes_df['ymax'] = bounding_boxes_df['ymax'] * self.height\n",
    "        \n",
    "        # bounding box to tensor\n",
    "        boxes = bounding_boxes_df[['xmin', 'ymin', 'xmax', 'ymax']].to_numpy()\n",
    "        labels = bounding_boxes_df['target'].to_numpy()\n",
    "        \n",
    "        is_negative_sample = len(boxes) == 0\n",
    "\n",
    "        if is_negative_sample:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.as_tensor(np.array([0]), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            # labels to tensor\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        # prepare the final `target` dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        # apply the image transforms\n",
    "        if not is_negative_sample:\n",
    "            sample = self.transforms(image = image_resized,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "        elif is_negative_sample:\n",
    "            sample = self.negative_sample_transforms(image=image_resized,\n",
    "                                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "        \n",
    "        return image_resized, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 8763\n",
      "Number of validation samples: 2803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare the final datasets and data loaders\n",
    "train_dataset = WormDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform(), get_negative_sample_transform())\n",
    "valid_dataset = WormDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform(), get_negative_sample_transform())\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sanity check of the Dataset pipeline with sample visualization\n",
    "# dataset = WormDataset(\n",
    "#     TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES\n",
    "# )\n",
    "# print(f\"Number of training images: {len(dataset)}\")\n",
    "\n",
    "# # function to visualize a single sample\n",
    "# def visualize_sample(image, target):\n",
    "#     box = target['boxes'][0] if len(target['boxes']) > 0 else []\n",
    "#     label = CLASSES[target['labels'][0]] if len(target['labels']) > 0 else []\n",
    "#     if len(box) != 0:\n",
    "#         cv2.rectangle(\n",
    "#             image, \n",
    "#             (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
    "#             (0, 255, 0), 1\n",
    "#         )\n",
    "#         cv2.putText(\n",
    "#             image, label, (int(box[0]), int(box[1]-5)), \n",
    "#             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n",
    "#         )\n",
    "#     cv2.imshow('Image', image)\n",
    "#     cv2.waitKey(0)\n",
    "    \n",
    "# NUM_SAMPLES_TO_VISUALIZE = 5\n",
    "# for i in range(NUM_SAMPLES_TO_VISUALIZE):\n",
    "#     image, target = dataset[i]\n",
    "#     visualize_sample(image, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "def create_model(num_classes):\n",
    "    \n",
    "    # load Faster RCNN pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    \n",
    "    # get the number of input features \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # define a new head for the detector with required number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running training iterations\n",
    "def train(train_data_loader, model, optimizer, train_loss_hist, train_loss_list):\n",
    "    print('Training')\n",
    "    \n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "\n",
    "        train_loss_hist.send(loss_value)\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        print(f\"Loss: {loss_value:.4f} ,Index: {i}, Total: {len(train_data_loader)}\")\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running validation iterations\n",
    "def validate(valid_data_loader, model, val_loss_hist, val_loss_list):\n",
    "    print('Validating')\n",
    "    \n",
    "    for i, data in enumerate(valid_data_loader):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "\n",
    "        val_loss_hist.send(loss_value)\n",
    "\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        print(f\"Loss: {loss_value:.4f} ,Index: {i}, Total: {len(valid_data_loader)}\")\n",
    "    return val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 100\n",
      "Training\n",
      "Loss: 5.6582 ,Index: 0, Total: 2191\n",
      "Loss: 1.5755 ,Index: 1, Total: 2191\n",
      "Loss: 0.7349 ,Index: 2, Total: 2191\n",
      "Loss: 1.1028 ,Index: 3, Total: 2191\n",
      "Loss: 1.2336 ,Index: 4, Total: 2191\n",
      "Loss: 0.5236 ,Index: 5, Total: 2191\n",
      "Loss: 0.6207 ,Index: 6, Total: 2191\n",
      "Loss: 1.2364 ,Index: 7, Total: 2191\n",
      "Loss: 0.8535 ,Index: 8, Total: 2191\n",
      "Loss: 1.0450 ,Index: 9, Total: 2191\n",
      "Loss: 0.6522 ,Index: 10, Total: 2191\n",
      "Loss: 0.7553 ,Index: 11, Total: 2191\n",
      "Loss: 0.6423 ,Index: 12, Total: 2191\n",
      "Loss: 1.8567 ,Index: 13, Total: 2191\n",
      "Loss: 1.6313 ,Index: 14, Total: 2191\n",
      "Loss: 1.2320 ,Index: 15, Total: 2191\n",
      "Loss: 0.4457 ,Index: 16, Total: 2191\n",
      "Loss: 0.9758 ,Index: 17, Total: 2191\n",
      "Loss: 0.2683 ,Index: 18, Total: 2191\n",
      "Loss: 0.9132 ,Index: 19, Total: 2191\n",
      "Loss: 0.7496 ,Index: 20, Total: 2191\n",
      "Loss: 0.4633 ,Index: 21, Total: 2191\n",
      "Loss: 0.8835 ,Index: 22, Total: 2191\n",
      "Loss: 0.5029 ,Index: 23, Total: 2191\n",
      "Loss: 0.5026 ,Index: 24, Total: 2191\n",
      "Loss: 0.7628 ,Index: 25, Total: 2191\n",
      "Loss: 0.8101 ,Index: 26, Total: 2191\n",
      "Loss: 0.8505 ,Index: 27, Total: 2191\n",
      "Loss: 1.3083 ,Index: 28, Total: 2191\n",
      "Loss: 0.7768 ,Index: 29, Total: 2191\n",
      "Loss: 0.1994 ,Index: 30, Total: 2191\n",
      "Loss: 1.0669 ,Index: 31, Total: 2191\n",
      "Loss: 1.3488 ,Index: 32, Total: 2191\n",
      "Loss: 1.1941 ,Index: 33, Total: 2191\n",
      "Loss: 0.6597 ,Index: 34, Total: 2191\n",
      "Loss: 0.7767 ,Index: 35, Total: 2191\n",
      "Loss: 0.9578 ,Index: 36, Total: 2191\n",
      "Loss: 0.8362 ,Index: 37, Total: 2191\n",
      "Loss: 0.8692 ,Index: 38, Total: 2191\n",
      "Loss: 0.3773 ,Index: 39, Total: 2191\n",
      "Loss: 0.5462 ,Index: 40, Total: 2191\n",
      "Loss: 1.2306 ,Index: 41, Total: 2191\n",
      "Loss: 0.3975 ,Index: 42, Total: 2191\n",
      "Loss: 0.3068 ,Index: 43, Total: 2191\n",
      "Loss: 0.4727 ,Index: 44, Total: 2191\n",
      "Loss: 0.8352 ,Index: 45, Total: 2191\n",
      "Loss: 0.3047 ,Index: 46, Total: 2191\n",
      "Loss: 0.6758 ,Index: 47, Total: 2191\n",
      "Loss: 0.6811 ,Index: 48, Total: 2191\n",
      "Loss: 0.8412 ,Index: 49, Total: 2191\n",
      "Loss: 1.0252 ,Index: 50, Total: 2191\n",
      "Loss: 0.7228 ,Index: 51, Total: 2191\n",
      "Loss: 1.5544 ,Index: 52, Total: 2191\n",
      "Loss: 0.7789 ,Index: 53, Total: 2191\n",
      "Loss: 1.0714 ,Index: 54, Total: 2191\n",
      "Loss: 0.5187 ,Index: 55, Total: 2191\n",
      "Loss: 0.5654 ,Index: 56, Total: 2191\n",
      "Loss: 0.6877 ,Index: 57, Total: 2191\n",
      "Loss: 0.6749 ,Index: 58, Total: 2191\n",
      "Loss: 0.4929 ,Index: 59, Total: 2191\n",
      "Loss: 0.9518 ,Index: 60, Total: 2191\n",
      "Loss: 0.7279 ,Index: 61, Total: 2191\n",
      "Loss: 0.6924 ,Index: 62, Total: 2191\n",
      "Loss: 0.7439 ,Index: 63, Total: 2191\n",
      "Loss: 0.8371 ,Index: 64, Total: 2191\n",
      "Loss: 0.0752 ,Index: 65, Total: 2191\n",
      "Loss: 0.5782 ,Index: 66, Total: 2191\n",
      "Loss: 0.2786 ,Index: 67, Total: 2191\n",
      "Loss: 0.1962 ,Index: 68, Total: 2191\n",
      "Loss: 0.2962 ,Index: 69, Total: 2191\n",
      "Loss: 0.3556 ,Index: 70, Total: 2191\n",
      "Loss: 0.5095 ,Index: 71, Total: 2191\n",
      "Loss: 0.7146 ,Index: 72, Total: 2191\n",
      "Loss: 1.0193 ,Index: 73, Total: 2191\n",
      "Loss: 0.2539 ,Index: 74, Total: 2191\n",
      "Loss: 1.0098 ,Index: 75, Total: 2191\n",
      "Loss: 0.8381 ,Index: 76, Total: 2191\n",
      "Loss: 0.5871 ,Index: 77, Total: 2191\n",
      "Loss: 1.2706 ,Index: 78, Total: 2191\n",
      "Loss: 0.8370 ,Index: 79, Total: 2191\n",
      "Loss: 0.3041 ,Index: 80, Total: 2191\n",
      "Loss: 0.3130 ,Index: 81, Total: 2191\n",
      "Loss: 0.6838 ,Index: 82, Total: 2191\n",
      "Loss: 0.7836 ,Index: 83, Total: 2191\n",
      "Loss: 0.5740 ,Index: 84, Total: 2191\n",
      "Loss: 0.9261 ,Index: 85, Total: 2191\n",
      "Loss: 0.9135 ,Index: 86, Total: 2191\n",
      "Loss: 0.8148 ,Index: 87, Total: 2191\n",
      "Loss: 1.0446 ,Index: 88, Total: 2191\n",
      "Loss: 0.6582 ,Index: 89, Total: 2191\n",
      "Loss: 0.5942 ,Index: 90, Total: 2191\n",
      "Loss: 0.5864 ,Index: 91, Total: 2191\n",
      "Loss: 0.6694 ,Index: 92, Total: 2191\n",
      "Loss: 0.7617 ,Index: 93, Total: 2191\n",
      "Loss: 0.2016 ,Index: 94, Total: 2191\n",
      "Loss: 0.1582 ,Index: 95, Total: 2191\n",
      "Loss: 0.3243 ,Index: 96, Total: 2191\n",
      "Loss: 0.4349 ,Index: 97, Total: 2191\n",
      "Loss: 0.4519 ,Index: 98, Total: 2191\n",
      "Loss: 0.5448 ,Index: 99, Total: 2191\n",
      "Loss: 0.5541 ,Index: 100, Total: 2191\n",
      "Loss: 0.4880 ,Index: 101, Total: 2191\n",
      "Loss: 0.6241 ,Index: 102, Total: 2191\n",
      "Loss: 0.4524 ,Index: 103, Total: 2191\n",
      "Loss: 0.3334 ,Index: 104, Total: 2191\n",
      "Loss: 0.6706 ,Index: 105, Total: 2191\n",
      "Loss: 0.2391 ,Index: 106, Total: 2191\n",
      "Loss: 0.4200 ,Index: 107, Total: 2191\n",
      "Loss: 0.6289 ,Index: 108, Total: 2191\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [128], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m# start timer and carry out training and validation\u001b[39;00m\n\u001b[1;32m     37\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 38\u001b[0m train_loss \u001b[39m=\u001b[39m train(train_loader, model, optimizer, train_loss_hist, train_loss_list)\n\u001b[1;32m     39\u001b[0m val_loss \u001b[39m=\u001b[39m validate(valid_loader, model, val_loss_hist, val_loss_list)\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch #\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m train loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss_hist\u001b[39m.\u001b[39mvalue\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)   \n",
      "Cell \u001b[0;32mIn [126], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_data_loader, model, optimizer, train_loss_hist, train_loss_list)\u001b[0m\n\u001b[1;32m      9\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(DEVICE) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[1;32m     10\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(DEVICE) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[0;32m---> 12\u001b[0m loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m     14\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     15\u001b[0m loss_value \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrpn(images, features, targets)\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroi_heads(features, proposals, images\u001b[39m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torchvision/models/detection/rpn.py:364\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39m# RPN uses all feature maps that are available\u001b[39;00m\n\u001b[1;32m    363\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(features\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m--> 364\u001b[0m objectness, pred_bbox_deltas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(features)\n\u001b[1;32m    365\u001b[0m anchors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manchor_generator(images, features)\n\u001b[1;32m    367\u001b[0m num_images \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(anchors)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torchvision/models/detection/rpn.py:76\u001b[0m, in \u001b[0;36mRPNHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m bbox_reg \u001b[39m=\u001b[39m []\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m x:\n\u001b[0;32m---> 76\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(feature)\n\u001b[1;32m     77\u001b[0m     logits\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_logits(t))\n\u001b[1;32m     78\u001b[0m     bbox_reg\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbbox_pred(t))\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# initialize the model and move to the computation device\n",
    "model = create_model(num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "# get the model parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# initialize the Averager class\n",
    "train_loss_hist = Averager()\n",
    "val_loss_hist = Averager()\n",
    "# train and validation loss lists to store loss values of all...\n",
    "# ... iterations till ena and plot graphs for all iterations\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# name to save the trained model with\n",
    "MODEL_NAME = 'rcnn'\n",
    "\n",
    "# whether to show transformed images from data loader or not\n",
    "if VISUALIZE_TRANSFORMED_IMAGES:\n",
    "    show_tranformed_image(train_loader)\n",
    "\n",
    "# start the training epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "\n",
    "    # reset the training and validation loss histories for the current epoch\n",
    "    train_loss_hist.reset()\n",
    "    val_loss_hist.reset()\n",
    "\n",
    "    # create two subplots, one for each, training and validation\n",
    "    figure_1, train_ax = plt.subplots()\n",
    "    figure_2, valid_ax = plt.subplots()\n",
    "\n",
    "    # start timer and carry out training and validation\n",
    "    start = time.time()\n",
    "    train_loss = train(train_loader, model, optimizer, train_loss_hist, train_loss_list)\n",
    "    val_loss = validate(valid_loader, model, val_loss_hist, val_loss_list)\n",
    "    print(f\"Epoch #{epoch} train loss: {train_loss_hist.value:.3f}\")   \n",
    "    print(f\"Epoch #{epoch} validation loss: {val_loss_hist.value:.3f}\")   \n",
    "    end = time.time()\n",
    "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "\n",
    "    if (epoch+1) % SAVE_MODEL_EPOCH == 0: # save model after every n epochs\n",
    "        torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n",
    "        print('SAVING MODEL COMPLETE...\\n')\n",
    "    \n",
    "    if (epoch+1) % SAVE_PLOTS_EPOCH == 0: # save loss plots after n epochs\n",
    "        train_ax.plot(train_loss, color='blue')\n",
    "        train_ax.set_xlabel('iterations')\n",
    "        train_ax.set_ylabel('train loss')\n",
    "        valid_ax.plot(val_loss, color='red')\n",
    "        valid_ax.set_xlabel('iterations')\n",
    "        valid_ax.set_ylabel('validation loss')\n",
    "        figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n",
    "        figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n",
    "        print('SAVING PLOTS COMPLETE...')\n",
    "    \n",
    "    if (epoch+1) == NUM_EPOCHS: # save loss plots and model once at the end\n",
    "        train_ax.plot(train_loss, color='blue')\n",
    "        train_ax.set_xlabel('iterations')\n",
    "        train_ax.set_ylabel('train loss')\n",
    "        valid_ax.plot(val_loss, color='red')\n",
    "        valid_ax.set_xlabel('iterations')\n",
    "        valid_ax.set_ylabel('validation loss')\n",
    "        figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n",
    "        figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n",
    "    \n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import glob as glob\n",
    "\n",
    "# set the computation device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# load the model and the trained weights\n",
    "model = create_model(num_classes=NUM_CLASSES).to(device)\n",
    "model.load_state_dict(torch.load(\n",
    "    '../outputs/model10.pth', map_location=device\n",
    "))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where all the images are present\n",
    "DIR_TEST = '../data/rcnn/training/'\n",
    "test_images = glob.glob(f\"{DIR_TEST}/*\")\n",
    "print(f\"Test instances: {len(test_images)}\")\n",
    "\n",
    "# define the detection threshold...\n",
    "# ... any detection having score below this will be discarded\n",
    "detection_threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(test_images)):\n",
    "    # get the image file name for saving output later on\n",
    "    image_name = test_images[i].split('/')[-1].split('.')[0]\n",
    "    image = cv2.imread(test_images[i])\n",
    "    orig_image = image.copy()\n",
    "    # BGR to RGB\n",
    "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    # make the pixel range between 0 and 1\n",
    "    image /= 255.0\n",
    "    # bring color channels to front\n",
    "    image = np.transpose(image, (2, 0, 1)).astype(np.float)\n",
    "    # convert to tensor\n",
    "    image = torch.tensor(image, dtype=torch.float).cuda()\n",
    "    # add batch dimension\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "    \n",
    "    # load all detection to CPU for further operations\n",
    "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "    # carry further only if there are detected boxes\n",
    "    if len(outputs[0]['boxes']) != 0:\n",
    "        boxes = outputs[0]['boxes'].data.numpy()\n",
    "        scores = outputs[0]['scores'].data.numpy()\n",
    "        # filter out boxes according to `detection_threshold`\n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        draw_boxes = boxes.copy()\n",
    "        # get all the predicited class names\n",
    "        pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "        \n",
    "        # draw the bounding boxes and write the class name on top of it\n",
    "        for j, box in enumerate(draw_boxes):\n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        (0, 0, 255), 2)\n",
    "            cv2.putText(orig_image, pred_classes[j], \n",
    "                        (int(box[0]), int(box[1]-5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "            results.append({\n",
    "                \"image\": test_images[i].split('/')[-1],\n",
    "                \"bounding_box\": box,\n",
    "                \"prediction\": pred_classes[j]\n",
    "            })\n",
    "\n",
    "        # cv2.imshow('Prediction', orig_image)\n",
    "        # cv2.waitKey(1)\n",
    "        cv2.imwrite(f\"../results/test_predictions/{image_name}.jpg\", orig_image,)\n",
    "    print(f\"Image {i+1} done...\")\n",
    "    print('-'*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('vision-next')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c12394e9c515c758c757d3f78baaf9271f06cd49f2ee5f6ea4c64548108e376c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
