{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "BATCH_SIZE = 64 # increase / decrease according to GPU memeory\n",
    "RESIZE_TO = 512 # resize the image for training and transforms\n",
    "NUM_EPOCHS = 10 # number of epochs to train for\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# training images and XML files directory\n",
    "TRAIN_DIR = '../data/rcnn/training'\n",
    "# validation images and XML files directory\n",
    "VALID_DIR = '../data/rcnn/validation'\n",
    "\n",
    "# classes: 0 index is reserved for background\n",
    "CLASSES = [\n",
    "    'background', 'abw', 'pbw'\n",
    "]\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# whether to visualize images after crearing the data loaders\n",
    "VISUALIZE_TRANSFORMED_IMAGES = False\n",
    "\n",
    "# location to save model and plots\n",
    "OUT_DIR = '../results/rcnn'\n",
    "SAVE_PLOTS_EPOCH = 2 # save loss plots after these many epochs\n",
    "SAVE_MODEL_EPOCH = 2 # save model after these many epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# this class keeps track of the training and validation loss values...\n",
    "# ... and helps to get the average for each epoch as well\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    To handle the data loading as different images may have different number \n",
    "    of objects and to handle varying size tensors as well.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training tranforms\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        A.RandomRotate90(0.5),\n",
    "        A.MotionBlur(p=0.2),\n",
    "        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        A.Blur(blur_limit=3, p=0.1),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc',\n",
    "        'label_fields': ['labels']\n",
    "    })\n",
    "\n",
    "# define the validation transforms\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc', \n",
    "        'label_fields': ['labels']\n",
    "    })\n",
    "\n",
    "# define the negative sample transforms\n",
    "def get_negative_sample_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tranformed_image(train_loader):\n",
    "    \"\"\"\n",
    "    This function shows the transformed images from the `train_loader`.\n",
    "    Helps to check whether the tranformed images along with the corresponding\n",
    "    labels are correct or not.\n",
    "    Only runs if `VISUALIZE_TRANSFORMED_IMAGES = True` in config.py.\n",
    "    \"\"\"\n",
    "    if len(train_loader) > 0:\n",
    "        for i in range(1):\n",
    "            images, targets = next(iter(train_loader))\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "            boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "            sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            for box in boxes:\n",
    "                cv2.rectangle(sample,\n",
    "                            (box[0], box[1]),\n",
    "                            (box[2], box[3]),\n",
    "                            (0, 0, 255), 2)\n",
    "            cv2.imshow('Transformed image', sample)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset class\n",
    "class WormDataset(Dataset):\n",
    "    def __init__(self, dir_path, width, height, classes, transforms, negative_sample_transforms):\n",
    "        self.transforms = transforms\n",
    "        self.negative_sample_transforms = negative_sample_transforms\n",
    "        self.dir_path = dir_path\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.classes = classes\n",
    "        \n",
    "        # get all the image paths in sorted order\n",
    "        self.image_paths = glob.glob(f\"{self.dir_path}/*[jpg|jpeg]\")\n",
    "        self.all_images = [image_path.split('/')[-1] for image_path in self.image_paths]\n",
    "        self.all_images = sorted(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # capture the image name and the full image path\n",
    "        image_name = self.all_images[idx]\n",
    "        image_path = os.path.join(self.dir_path, image_name)\n",
    "\n",
    "        # read the image\n",
    "        image = cv2.imread(image_path)\n",
    "        # convert BGR to RGB color format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image_resized = cv2.resize(image, (self.width, self.height))\n",
    "        image_resized /= 255.0\n",
    "        \n",
    "        # capture the corresponding XML file for getting the annotations\n",
    "        annotations_file = image_name.split('.')[0] + '.txt'\n",
    "        annotations_file_path = os.path.join(self.dir_path, annotations_file)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        # get the height and width of the image\n",
    "        image_width = image.shape[1]\n",
    "        image_height = image.shape[0]\n",
    "        \n",
    "        # box coordinates for xml files are extracted and corrected for image size given\n",
    "\n",
    "        bounding_boxes_df = pd.read_csv(annotations_file_path, sep='\\t', names=[\"target\", 'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "        bounding_boxes_df['xmin'] = bounding_boxes_df['xmin'] * self.width\n",
    "        bounding_boxes_df['xmax'] = bounding_boxes_df['xmax'] * self.width\n",
    "        bounding_boxes_df['ymin'] = bounding_boxes_df['ymin'] * self.height\n",
    "        bounding_boxes_df['ymax'] = bounding_boxes_df['ymax'] * self.height\n",
    "        \n",
    "        # bounding box to tensor\n",
    "        boxes = bounding_boxes_df[['xmin', 'ymin', 'xmax', 'ymax']].to_numpy()\n",
    "        labels = bounding_boxes_df['target'].to_numpy()\n",
    "        \n",
    "        is_negative_sample = len(boxes) == 0\n",
    "\n",
    "        if is_negative_sample:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.as_tensor(np.array([0]), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            # labels to tensor\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        # prepare the final `target` dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        # apply the image transforms\n",
    "        if not is_negative_sample:\n",
    "            sample = self.transforms(image = image_resized,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "        elif is_negative_sample:\n",
    "            sample = self.negative_sample_transforms(image=image_resized,\n",
    "                                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "        \n",
    "        return image_resized, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 8763\n",
      "Number of validation samples: 974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare the final datasets and data loaders\n",
    "train_dataset = WormDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform(), get_negative_sample_transform())\n",
    "valid_dataset = WormDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform(), get_negative_sample_transform())\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sanity check of the Dataset pipeline with sample visualization\n",
    "# dataset = WormDataset(\n",
    "#     TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES\n",
    "# )\n",
    "# print(f\"Number of training images: {len(dataset)}\")\n",
    "\n",
    "# # function to visualize a single sample\n",
    "# def visualize_sample(image, target):\n",
    "#     box = target['boxes'][0] if len(target['boxes']) > 0 else []\n",
    "#     label = CLASSES[target['labels'][0]] if len(target['labels']) > 0 else []\n",
    "#     if len(box) != 0:\n",
    "#         cv2.rectangle(\n",
    "#             image, \n",
    "#             (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
    "#             (0, 255, 0), 1\n",
    "#         )\n",
    "#         cv2.putText(\n",
    "#             image, label, (int(box[0]), int(box[1]-5)), \n",
    "#             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n",
    "#         )\n",
    "#     cv2.imshow('Image', image)\n",
    "#     cv2.waitKey(0)\n",
    "    \n",
    "# NUM_SAMPLES_TO_VISUALIZE = 5\n",
    "# for i in range(NUM_SAMPLES_TO_VISUALIZE):\n",
    "#     image, target = dataset[i]\n",
    "#     visualize_sample(image, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "def create_model(num_classes):\n",
    "    \n",
    "    # load Faster RCNN pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    \n",
    "    # get the number of input features \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # define a new head for the detector with required number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running training iterations\n",
    "def train(train_data_loader, model, optimizer, train_loss_hist, train_loss_list):\n",
    "    print('Training')\n",
    "    \n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "\n",
    "        train_loss_hist.send(loss_value)\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        print(f\"Loss: {loss_value:.4f} ,Index: {i}, Total: {len(train_data_loader)}\")\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running validation iterations\n",
    "def validate(valid_data_loader, model, val_loss_hist, val_loss_list):\n",
    "    print('Validating')\n",
    "    \n",
    "    for i, data in enumerate(valid_data_loader):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "\n",
    "        val_loss_hist.send(loss_value)\n",
    "\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        print(f\"Loss: {loss_value:.4f} ,Index: {i}, Total: {len(valid_data_loader)}\")\n",
    "    return val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 10\n",
      "Training\n",
      "Loss: 2.9308 ,Index: 0, Total: 137\n",
      "Loss: 2.3829 ,Index: 1, Total: 137\n",
      "Loss: 1.4418 ,Index: 2, Total: 137\n",
      "Loss: 0.9899 ,Index: 3, Total: 137\n",
      "Loss: 0.9450 ,Index: 4, Total: 137\n",
      "Loss: 0.9439 ,Index: 5, Total: 137\n",
      "Loss: 0.9372 ,Index: 6, Total: 137\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m# start timer and carry out training and validation\u001b[39;00m\n\u001b[1;32m     37\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 38\u001b[0m train_loss \u001b[39m=\u001b[39m train(train_loader, model, optimizer, train_loss_hist, train_loss_list)\n\u001b[1;32m     39\u001b[0m val_loss \u001b[39m=\u001b[39m validate(valid_loader, model, val_loss_hist, val_loss_list)\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch #\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m train loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss_hist\u001b[39m.\u001b[39mvalue\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)   \n",
      "Cell \u001b[0;32mIn [12], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_data_loader, model, optimizer, train_loss_hist, train_loss_list)\u001b[0m\n\u001b[1;32m      9\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(DEVICE) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[1;32m     10\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(DEVICE) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[0;32m---> 12\u001b[0m loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m     14\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     15\u001b[0m loss_value \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torchvision/models/detection/generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m             degen_bb: List[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m boxes[bb_idx]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     95\u001b[0m             torch\u001b[39m.\u001b[39m_assert(\n\u001b[1;32m     96\u001b[0m                 \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAll bounding boxes should have positive height and width.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Found invalid box \u001b[39m\u001b[39m{\u001b[39;00mdegen_bb\u001b[39m}\u001b[39;00m\u001b[39m for target at index \u001b[39m\u001b[39m{\u001b[39;00mtarget_idx\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m             )\n\u001b[0;32m--> 101\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensors)\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torchvision/models/detection/backbone_utils.py:57\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Tensor]:\n\u001b[0;32m---> 57\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbody(x)\n\u001b[1;32m     58\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfpn(x)\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torchvision/models/_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m out \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m name, module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 69\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers:\n\u001b[1;32m     71\u001b[0m         out_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torchvision/models/resnet.py:154\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m--> 154\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(out)\n\u001b[1;32m    155\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/vision/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the model and move to the computation device\n",
    "model = create_model(num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "# get the model parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# initialize the Averager class\n",
    "train_loss_hist = Averager()\n",
    "val_loss_hist = Averager()\n",
    "# train and validation loss lists to store loss values of all...\n",
    "# ... iterations till ena and plot graphs for all iterations\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# name to save the trained model with\n",
    "MODEL_NAME = 'rcnn'\n",
    "\n",
    "# whether to show transformed images from data loader or not\n",
    "if VISUALIZE_TRANSFORMED_IMAGES:\n",
    "    show_tranformed_image(train_loader)\n",
    "\n",
    "# start the training epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "\n",
    "    # reset the training and validation loss histories for the current epoch\n",
    "    train_loss_hist.reset()\n",
    "    val_loss_hist.reset()\n",
    "\n",
    "    # create two subplots, one for each, training and validation\n",
    "    figure_1, train_ax = plt.subplots()\n",
    "    figure_2, valid_ax = plt.subplots()\n",
    "\n",
    "    # start timer and carry out training and validation\n",
    "    start = time.time()\n",
    "    train_loss = train(train_loader, model, optimizer, train_loss_hist, train_loss_list)\n",
    "    val_loss = validate(valid_loader, model, val_loss_hist, val_loss_list)\n",
    "    print(f\"Epoch #{epoch} train loss: {train_loss_hist.value:.3f}\")   \n",
    "    print(f\"Epoch #{epoch} validation loss: {val_loss_hist.value:.3f}\")   \n",
    "    end = time.time()\n",
    "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "\n",
    "    if (epoch+1) % SAVE_MODEL_EPOCH == 0: # save model after every n epochs\n",
    "        torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n",
    "        print('SAVING MODEL COMPLETE...\\n')\n",
    "    \n",
    "    if (epoch+1) % SAVE_PLOTS_EPOCH == 0: # save loss plots after n epochs\n",
    "        train_ax.plot(train_loss, color='blue')\n",
    "        train_ax.set_xlabel('iterations')\n",
    "        train_ax.set_ylabel('train loss')\n",
    "        valid_ax.plot(val_loss, color='red')\n",
    "        valid_ax.set_xlabel('iterations')\n",
    "        valid_ax.set_ylabel('validation loss')\n",
    "        figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n",
    "        figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n",
    "        print('SAVING PLOTS COMPLETE...')\n",
    "    \n",
    "    if (epoch+1) == NUM_EPOCHS: # save loss plots and model once at the end\n",
    "        train_ax.plot(train_loss, color='blue')\n",
    "        train_ax.set_xlabel('iterations')\n",
    "        train_ax.set_ylabel('train loss')\n",
    "        valid_ax.plot(val_loss, color='red')\n",
    "        valid_ax.set_xlabel('iterations')\n",
    "        valid_ax.set_ylabel('validation loss')\n",
    "        figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n",
    "        figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n",
    "    \n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import glob as glob\n",
    "\n",
    "# set the computation device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# load the model and the trained weights\n",
    "model = create_model(num_classes=NUM_CLASSES).to(device)\n",
    "model.load_state_dict(torch.load(\n",
    "    '../outputs/model10.pth', map_location=device\n",
    "))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where all the images are present\n",
    "DIR_TEST = '../data/rcnn/testing/'\n",
    "test_images = glob.glob(f\"{DIR_TEST}/*\")\n",
    "print(f\"Test instances: {len(test_images)}\")\n",
    "\n",
    "# define the detection threshold...\n",
    "# ... any detection having score below this will be discarded\n",
    "detection_threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(test_images)):\n",
    "    # get the image file name for saving output later on\n",
    "    image_name = test_images[i].split('/')[-1].split('.')[0]\n",
    "    image = cv2.imread(test_images[i])\n",
    "    orig_image = image.copy()\n",
    "    # BGR to RGB\n",
    "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    # make the pixel range between 0 and 1\n",
    "    image /= 255.0\n",
    "    # bring color channels to front\n",
    "    image = np.transpose(image, (2, 0, 1)).astype(np.float)\n",
    "    # convert to tensor\n",
    "    image = torch.tensor(image, dtype=torch.float).cuda()\n",
    "    # add batch dimension\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "    \n",
    "    # load all detection to CPU for further operations\n",
    "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "    # carry further only if there are detected boxes\n",
    "    if len(outputs[0]['boxes']) != 0:\n",
    "        boxes = outputs[0]['boxes'].data.numpy()\n",
    "        scores = outputs[0]['scores'].data.numpy()\n",
    "        # filter out boxes according to `detection_threshold`\n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        draw_boxes = boxes.copy()\n",
    "        # get all the predicited class names\n",
    "        pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "        \n",
    "        # draw the bounding boxes and write the class name on top of it\n",
    "        for j, box in enumerate(draw_boxes):\n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        (0, 0, 255), 2)\n",
    "            cv2.putText(orig_image, pred_classes[j], \n",
    "                        (int(box[0]), int(box[1]-5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "            results.append({\n",
    "                \"image\": test_images[i].split('/')[-1],\n",
    "                \"bounding_box\": box,\n",
    "                \"prediction\": pred_classes[j]\n",
    "            })\n",
    "\n",
    "        # cv2.imshow('Prediction', orig_image)\n",
    "        # cv2.waitKey(1)\n",
    "        cv2.imwrite(f\"../results/test_predictions/{image_name}.jpg\", orig_image,)\n",
    "    print(f\"Image {i+1} done...\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54557d5da701f14d6acbec16753c0b2a30b0c175a2c97d182468fef3f37829cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
